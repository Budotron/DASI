These data were collected over the years 1972 to 2012. According to Appendix A of the GSS Codebook for Cumulative Data, the data were collected by survey conducted via a multi-stage random sample, which blocked for sex, age, and employment status, as well as race and income levels. There were three modes of data collection: computer assisted personal interviewing (CAPI), face-to-face interviews, and telephone interviews. The interviewer was obliged to meet a quota for each blocking level, and, to minimize the introduction of convenience sampling bias (by neglecting to survey those at work or school), they were instructed to conduct their surveys after 1500 hours on weekdays, or on weekends and holidays. Each observation was an individual adult, living in the area scoped by the surveyor. In data collected before 2006, these individuals were exclusively English speakers; subsequent years included Spanish-speaking individuals as well.
The respondents were interviewed about their background and families, and provided responses on a Likert Scale for Personal, Societal, Economic and Workplace concerns, as well as their opinion on controversial social issues. In total, each respondant in the modified data set replied to well over 100 questions. This study limits itself to the examination of only two of those variables, identified in the GSS codebook by "maeduc" and "agekdbrn", which respectively correspond to the number of years of education received by the respondent's mother (from 0 to 20), and the age at which the respondent had his or her first child. Both of these data are numerical, the latter being continuous, and the former discrete.
The population of interest for this study is all English- and Spanish-speaking United States residents, who are over 18 and not institutionalized. The geographic coverage is the entire United States. This is a retrospective observational study, and because the samples are randomly selected from the population of interest, the results are generalizable to the population. The conclusions of this study cannot be used to establish causility, as this is only a sample-based study. Even though the data sampling methods were designed to minimize convenience sampling bias, they may still suffer from non-response and voluntary response biases, as the interviewer's quotas may be filled with the responses of individuals who have strong opinions on some of the Controversial Social Issues questions, while those unwilling to share their background information may be underrepresented.
### Exploratory data analysis:
The data were subsetted to only include responses from 2006 onwards, the year from which Spanish-speaking residents were included in the survey, and for which the "childs" variable (which indicate the number of children of the respondent) was at least one. From this subset, a smaller dataset was extracted, including for each individual their maternal educational level and their age when their first child was born.
```{r, message=FALSE}
# load required packages
packages<-c("Hmisc", "dplyr", "reshape2", "ggplot2")
sapply(packages, require, character.only = TRUE)
```
```{r, cache=TRUE}
# load the gss data, unless this step has already been done
if(!exists("gss")){
load(url("http://bit.ly/dasi_gss_data"))
}
# subset gss to only include data after 2006 and for which respondents have more than 0 children
gss2<-gss[gss$year>=2006 & gss$childs >1, ]
# select the variables of interest from the subsetted data set
gss_small<-select(gss2, maeduc, agekdbrn)
# remove missing values
gss_small<-na.omit(gss_small)
qplot(x = maeduc, y = agekdbrn, data = gss_small, geom = c("point", "smooth"), method ="lm", xlab = "Maternal Education (Years)", ylab = "Age of respondent when first child was born")
```
The fitted line indicates a slight positive relationship between maternal education, and the age at which the respondents' first child was born. The maeduc variable was converted from a discrete numerical variable to a categorical varaible to facilitate further analysis
```{r}
gss_small$maeduc<-cut2(gss_small$maeduc, c(9, 13, 15))
levels(gss_small$maeduc)
g1<-ggplot(gss_small, aes(factor(maeduc), agekdbrn))
g1+ geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)
```
In cobmination, the boxplots and the denstiy plots show that the mean age at which a woman has her first child increases slightly as the level her mother's education increases. As the widespread higher education of women is a relatively new phenomenon, we would expect that there would be fewer grandmothers with the highest educational level; and indeed, there is a precipitous drop between the number of grandmothers with fewer than fifteen years of education and the number of grandmothers with more than fifteen years. The distributions of the density curves shows that for each category, the distributions are more-or-less normal with extremely small variance, ans more-or-less unimodal, the density plot for individuals with mothers educated beyond 15 years being the least so.
### Inference:
Insert inference section here...
### Conclusion:
Insert conclusion here...
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)+ xlab = "Maternal Education (Years)"
ggplot(gss_small, aes(factor(maeduc), agekdbrn)) + geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
ggplot(gss_small, aes(factor(maeduc), agekdbrn)) + geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)+ xlab = "Maternal Education (Years)"
```
ggplot(gss_small, aes(factor(maeduc), agekdbrn)) + geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)
```
gss_small$maeduc<-cut2(gss_small$maeduc, c(9, 13, 15))
levels(gss_small$maeduc)
ggplot(gss_small, aes(factor(maeduc), agekdbrn)) + geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)
```
ggplot(data = gss_small, aes(agekdbrn, colour = maeduc)) + geom_density() + facet_grid(.~maeduc)
ggplot(gss_small, aes(factor(maeduc), agekdbrn)) + geom_boxplot() + xlab("Maternal Education ") + ylab("Age of respondent when first child was born")
---
title: "RMQuiz2"
author: "Varun Boodram"
date: "October 18, 2014"
output: html_document
---
Question 1
Consider the following data with x as the predictor and y as as the outcome.
```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
```
Give a P-value for the two sided hypothesis test of whether $\beta1$ from a linear regression model is 0 or not.
```{r}
data<-data.frame(cbind(x, y))
# lm(outcome, predictor)
regr<-lm(formula = y~x, data)
summary(regr)
```
---
title: "RMQuiz2"
author: "Varun Boodram"
date: "October 18, 2014"
output: html_document
---
Question 1
Consider the following data with x as the predictor and y as as the outcome.
```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
```
Give a P-value for the two sided hypothesis test of whether $\beta1$ from a linear regression model is 0 or not.
```{r}
data<-data.frame(cbind(x, y))
# lm(outcome, predictor)
regr<-lm(formula = y~x, data)
summary(regr)$Coefficients
```
In the summary
summary(regr)$Coefficients
class(summary(regr))
summary(regr)
coef(summary(regr))
summary(regr)
res(summary(regr))
residual(summary(regr))
summary(regr)$Residual
summary(regr)$r.squared
names(summary)
summary(regr)
summary(regr)$sigma
fit<-lm(formula = mpg~wt, data = mtcars)
fit
ave(mtcars$wt)
mtcars$wt
c(unique(ave(mtcars$wt)), mean(mtcars$wt))
averageWeight<-mean(mtcars$wt)
averageWeight
summary(fit)
fit$coef[1]
fit$coef[2,1]
fit$coef[2]
source('~/.active-rstudio-document', echo=TRUE)
summary(fit)
fit$coef[4]
fit$coef[3]
fit$coef
fit$coefficients
fit$df
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
b0<-fit$coef[1]; b0
b1<-fit$coef[2]; b1
point<-b0+b1*averageWeight; point
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
qt(p = 0.975, df = fit$df)
plot(mtcars$mpg, mtcars$wt)
averageWeight
37.29+(-5.344*averageWeight)
plot(mtcars$mpg, mtcars$wt)
abline(fit)
plot(mtcars$mpg, mtcars$wt)
abline(fit, lwd=3, color="red")
plot(mtcars$mpg, mtcars$wt)
abline(fit, lwd=3, col="red")
plot(x = mpg, y = wt, data=mtcars)
abline(fit, lwd=3, col="red")
plot( mpg~wt, data=mtcars)
abline(fit, lwd=3, col="red")
?mtcars
newWeight<-3000/1000
```
and predict the mpg with
```{r}
predict(object = fit, newdata = data.frame(wt = newWeight))
fit2<-lm(mpg~1000*wt, mtcars)
fit2<-lm(mpg~(1000*wt), mtcars)
fit2<-lm(formula = mpg~(1000*wt), data = mtcars)
1000*mtcars$wt
df<-cbind(mtcars$mpg, 1000*mtcars$wt)
df
colnames(df<-mpg, wt)
colnames(df)<-c("mpg", "wt")
fit2<-lm(formula = mpg~wt, data = df)
class(df)
df<-data.frame(cbind(mtcars$mpg, 1000*mtcars$wt))
df
colnames(df)<-c("mpg", "wt")
fit2<-lm(formula = mpg~wt, data = df)
predMpg<-predict(object = fit2, newdata = data.frame(wt = newWeight))
summary(fit2)
predMpg+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*2*0.5591
T<-(b1-0)/0.5591
T
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
rm(list = ls())
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
```
---
title: "RMQuiz2"
author: "Varun Boodram"
date: "October 18, 2014"
output: html_document
---
Question 1
Consider the following data with x as the predictor and y as as the outcome.
```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
```
Give a P-value for the two sided hypothesis test of whether $\beta1$ from a linear regression model is 0 or not.
```{r}
data<-data.frame(cbind(x, y))
# lm(outcome, predictor)
regr<-lm(formula = y~x, data)
coef(summary(regr))
```
In the coefficient summary, under Pr(>|t|), we can find the desired value.
Question 2
Consider the previous problem, give the estimate of the residual standard deviation.
The desired value can be read off the third-to-last line of
```{r}
summary(regr)
```
or it can be called directly with
```{r}
summary(regr)$sigma
```
Question 3
In the ```mtcars``` data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint?
```{r}
# fit regression line
fit<-lm(formula = mpg~wt, data = mtcars)
averageWeight<-mean(mtcars$wt)
```
Consider developing a probabilistic model for linear regression
$$Y_i=\beta_0+\beta_1X_i+\epsilon$$
where the$\epsilon$ are assumed iid $N\sim(0,1)$.Because of this, the conditional expectations of the Ys given the Xs, which we denote $\mu_i$ is
$$E[Y_i\vert X_i=x_i]=\mu_i=\beta_0+\beta_1x_i$$ This is easy to see: take the expected values
$$
\begin{aligned}
E[Y_i\vert X_i=x_i]&=E[\beta_0+\beta_1X_i+\epsilon]\\
&=\beta_0+\beta_1X_i+E[\epsilon]\\
&=\beta_0+\beta_1X_i+0
\end{aligned}
$$
The point estimate for the outcome (mpg) at the average value of the predictor (wt) is $$\beta_0+\beta_1\bar{X}$$
```{r}
b0<-fit$coef[1]; b0
b1<-fit$coef[2]; b1
point<-b0+b1*averageWeight; point
```
Use the formula
$$
\begin{aligned}
95\%\text{CI}&=\text{point est}\pm ME\\
&=\text{point} \pm t_{tf}^*SE_{b_1}
\end{aligned}
$$
```{r}
point+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
```
The lower endpoint is larger than the higher. This is because mpg and wt are inversely corelated
```{r}
plot( mpg~wt, data=mtcars)
abline(fit, lwd=3, col="red")
```
Question 4
Refer to the previous question. Read the help file for mtcars. What is the weight coefficient interpreted as?
According to ```?mtcars```, the variable wt is Weight (lb/1000). Thus the weight coefficient is The estimated expected change in mpg per 1,000 lb increase in weight.
Question 5
Consider again the ```mtcars``` data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint?
The weight variable is is in units of (lb/1000). Define
```{r}
newWeight<-3000/1000
```
and predict the mpg with
```{r}
# including the interval argument gives the 95% CI (default) upper and lower bounds for a predicted value
predMpg<-predict(object = fit, newdata = data.frame(wt = newWeight), interval = "predict"); predMpg
```
Question 6
Consider again the ```mtcars``` data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint.
Consider the impact of changing the units of X. Therefore, multiplication of $X$ by a factor $\alpha$ results in dividing the coefficient by a factor of $\alpha$.
$$
\begin{aligned}
Y_i&=\beta_0+\beta_1X_i+\epsilon_i\\
&=\beta_0+\frac{\beta_1}{\alpha}(\alpha X_i)+\epsilon_i\\
\end{aligned}
$$
Conversion to short tons is equivalent to multiplication of $X$ by $1/2$, so $\beta_1$ is scaled by a factor of 2.
```{r}
b1<-2*b1
```
To calculate the Confidence interval for the slope, use the same reasoning as in question 3
$$
\begin{aligned}
95\%\text{CI}&=\text{point est}\pm ME\\
&=\text{b_1} \pm t_{tf}^*SE_{b_1}
\end{aligned}
$$
```{r}
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
```
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
(b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591)/2
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
---
title: "RMQuiz2"
author: "Varun Boodram"
date: "October 18, 2014"
output: html_document
---
Question 1
Consider the following data with x as the predictor and y as as the outcome.
```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
```
Give a P-value for the two sided hypothesis test of whether $\beta1$ from a linear regression model is 0 or not.
```{r}
data<-data.frame(cbind(x, y))
# lm(outcome, predictor)
regr<-lm(formula = y~x, data)
coef(summary(regr))
```
In the coefficient summary, under Pr(>|t|), we can find the desired value.
Question 2
Consider the previous problem, give the estimate of the residual standard deviation.
The desired value can be read off the third-to-last line of
```{r}
summary(regr)
```
or it can be called directly with
```{r}
summary(regr)$sigma
```
Question 3
In the ```mtcars``` data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint?
```{r}
# fit regression line
fit<-lm(formula = mpg~wt, data = mtcars)
averageWeight<-mean(mtcars$wt)
```
Consider developing a probabilistic model for linear regression
$$Y_i=\beta_0+\beta_1X_i+\epsilon$$
where the$\epsilon$ are assumed iid $N\sim(0,1)$.Because of this, the conditional expectations of the Ys given the Xs, which we denote $\mu_i$ is
$$E[Y_i\vert X_i=x_i]=\mu_i=\beta_0+\beta_1x_i$$ This is easy to see: take the expected values
$$
\begin{aligned}
E[Y_i\vert X_i=x_i]&=E[\beta_0+\beta_1X_i+\epsilon]\\
&=\beta_0+\beta_1X_i+E[\epsilon]\\
&=\beta_0+\beta_1X_i+0
\end{aligned}
$$
The point estimate for the outcome (mpg) at the average value of the predictor (wt) is $$\beta_0+\beta_1\bar{X}$$
```{r}
b0<-fit$coef[1]; b0
b1<-fit$coef[2]; b1
point<-b0+b1*averageWeight; point
```
Use the formula
$$
\begin{aligned}
95\%\text{CI}&=\text{point est}\pm ME\\
&=\text{point} \pm t_{tf}^*SE_{b_1}
\end{aligned}
$$
```{r}
point+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
```
The lower endpoint is larger than the higher. This is because mpg and wt are inversely corelated
```{r}
plot( mpg~wt, data=mtcars)
abline(fit, lwd=3, col="red")
```
Question 4
Refer to the previous question. Read the help file for mtcars. What is the weight coefficient interpreted as?
According to ```?mtcars```, the variable wt is Weight (lb/1000). Thus the weight coefficient is The estimated expected change in mpg per 1,000 lb increase in weight.
Question 5
Consider again the ```mtcars``` data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint?
The weight variable is is in units of (lb/1000). Define
```{r}
newWeight<-3000/1000
```
and predict the mpg with
```{r}
# including the interval argument gives the 95% CI (default) upper and lower bounds for a predicted value
predMpg<-predict(object = fit, newdata = data.frame(wt = newWeight), interval = "predict"); predMpg
```
Question 6
Consider again the ```mtcars``` data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint.
Consider the impact of changing the units of X. Therefore, multiplication of $X$ by a factor $\alpha$ results in dividing the coefficient by a factor of $\alpha$.
$$
\begin{aligned}
Y_i&=\beta_0+\beta_1X_i+\epsilon_i\\
&=\beta_0+\frac{\beta_1}{\alpha}(\alpha X_i)+\epsilon_i\\
\end{aligned}
$$
Conversion to short tons is equivalent to multiplication of $X$ by $1/2$, so $\beta_1$ is scaled by a factor of 2.
```{r}
b1<-2*b1
```
To calculate the Confidence interval for the slope, use the same reasoning as in question 3
$$
\begin{aligned}
95\%\text{CI}&=\text{point est}\pm ME\\
&=\text{b_1} \pm t_{tf}^*SE_{b_1}
\end{aligned}
$$
```{r}
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
```
b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591
2*(b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591)
.5*(b1+c(1,-1)*qt(p = 0.975, df = fit$df)*0.5591)
meanOnly<-mean(mpg)
```
anova gies the sum quared errors
```{r}
aov(meanOnly, fit)
meanOnly<-mean(mtcars$mpg)
```
anova gies the sum quared errors
```{r}
aov(meanOnly, fit)
```{r}
meanOnly<-lm(mpg~wt-1, data = mtcars); meanOnly
```
anova gies the sum quared errors
```{r}
aov(meanOnly, fit)
meanOnly
summary(meanOnly)
meanOnly<-lm(mpg~wt-1, data = mtcars); meanOnly
```
anova gives the sum quared errors
```{r}
annova(meanOnly, fit)
meanOnly<-lm(mpg~wt-1, data = mtcars); meanOnly
```
anova gives the sum quared errors
```{r}
anova(meanOnly, fit)
meanOnly<-lm(mpg~wt-1, data = mtcars); meanOnly
```
anova gives the sum quared errors
```{r}
anov<-anova(meanOnly, fit)
summary(anov)
summary(meanOnly)
anova(meanOnly, fit)
meanOnly<-lm(mpg~1, data = mtcars); meanOnly
mean(mtcars$mpg)
meanOnly<-lm(mpg~1, data = mtcars); meanOnly
```
anova gives the sum quared errors
```{r}
anova(meanOnly, fit)
summary(meanOnly)
278/1126
library(knitr)
knit("RMQuiz2.Rmd")
setwd("~/Desktop/RegressionQuizzes")
knit("RMQuiz2.Rmd")
---
title: "Slidify"
author: "Varun Boodram"
date: "October 19, 2014"
output: html_document
---
Slidify is a way to create data-centric presentations. It is an amalgamation of knitr, markdown, several js libraries for HTML5.
Installing
```{r}
install.packages("devtools")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
library(slidify)
```
setwd("~/Desktop/DataProductsWk2")
---
title: "Slidify"
author: "Varun Boodram"
date: "October 19, 2014"
output: html_document
---
Slidify is a way to create data-centric presentations. It is an amalgamation of knitr, markdown, several js libraries for HTML5.
Installing
```{r}
install.packages("devtools")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
library(slidify)
```
Start an empty slidify doc with author
```{r}
author("first_deck")
---
title: "Slidify"
author: "Varun Boodram"
date: "October 19, 2014"
output: html_document
---
Slidify is a way to create data-centric presentations. It is an amalgamation of knitr, markdown, several js libraries for HTML5.
Installing
```
install.packages("devtools")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
install.packages("devtools")
library(slidify)
author('test')
slidify('index.Rmd')
library(knitr)
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
setwd("~/Desktop/DataProductsWk2")
library(slidify)
author('test')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
slidify("index.Rmd")
browseURL('index.html')
slidify("index.Rmd")
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
browseURL('index.html')
knit()
setwd("~/Desktop/DASI")
knit("dasi_project_template.Rmd")
